{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/data/home/umang/Trajectory_project/anomaly_traj_data/numosim/'\n",
    "train_file_name = 'stay_points_train.parquet'\n",
    "poi_filename = 'poi.parquet'\n",
    "train_df = pd.read_parquet(f'{data_folder}/{train_file_name}')\n",
    "poi_df = pd.read_parquet(f'{data_folder}/{poi_filename}')\n",
    "train_poi_df = pd.merge(train_df, poi_df, on='poi_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 00:00:00-08:00</td>\n",
       "      <td>2024-01-01 11:36:59-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1912553</td>\n",
       "      <td>2024-01-01 11:51:32-08:00</td>\n",
       "      <td>2024-01-01 12:33:47-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 12:48:23-08:00</td>\n",
       "      <td>2024-01-01 13:06:35-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>103611</td>\n",
       "      <td>2024-01-01 13:22:50-08:00</td>\n",
       "      <td>2024-01-01 14:05:50-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 14:21:28-08:00</td>\n",
       "      <td>2024-01-01 15:22:44-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id   poi_id            start_datetime              end_datetime\n",
       "0         1  1518791 2024-01-01 00:00:00-08:00 2024-01-01 11:36:59-08:00\n",
       "1         1  1912553 2024-01-01 11:51:32-08:00 2024-01-01 12:33:47-08:00\n",
       "2         1  1518791 2024-01-01 12:48:23-08:00 2024-01-01 13:06:35-08:00\n",
       "3         1   103611 2024-01-01 13:22:50-08:00 2024-01-01 14:05:50-08:00\n",
       "4         1  1518791 2024-01-01 14:21:28-08:00 2024-01-01 15:22:44-08:00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all column names to lowercase\n",
    "train_df.columns = train_df.columns.str.lower()\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 00:00:00-08:00</td>\n",
       "      <td>2024-01-01 11:36:59-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1912553</td>\n",
       "      <td>2024-01-01 11:51:32-08:00</td>\n",
       "      <td>2024-01-01 12:33:47-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 12:48:23-08:00</td>\n",
       "      <td>2024-01-01 13:06:35-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>103611</td>\n",
       "      <td>2024-01-01 13:22:50-08:00</td>\n",
       "      <td>2024-01-01 14:05:50-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 14:21:28-08:00</td>\n",
       "      <td>2024-01-01 15:22:44-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261433</th>\n",
       "      <td>200000</td>\n",
       "      <td>329705</td>\n",
       "      <td>2024-01-27 08:24:08-08:00</td>\n",
       "      <td>2024-01-27 09:24:42-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261434</th>\n",
       "      <td>200000</td>\n",
       "      <td>408965</td>\n",
       "      <td>2024-01-27 09:38:34-08:00</td>\n",
       "      <td>2024-01-27 10:19:04-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261435</th>\n",
       "      <td>200000</td>\n",
       "      <td>402449</td>\n",
       "      <td>2024-01-27 10:30:15-08:00</td>\n",
       "      <td>2024-01-27 13:07:01-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261436</th>\n",
       "      <td>200000</td>\n",
       "      <td>337944</td>\n",
       "      <td>2024-01-27 13:21:36-08:00</td>\n",
       "      <td>2024-01-27 14:09:46-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261437</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-27 14:26:30-08:00</td>\n",
       "      <td>2024-01-29 07:34:54-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17261438 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agent_id   poi_id            start_datetime  \\\n",
       "0                1  1518791 2024-01-01 00:00:00-08:00   \n",
       "1                1  1912553 2024-01-01 11:51:32-08:00   \n",
       "2                1  1518791 2024-01-01 12:48:23-08:00   \n",
       "3                1   103611 2024-01-01 13:22:50-08:00   \n",
       "4                1  1518791 2024-01-01 14:21:28-08:00   \n",
       "...            ...      ...                       ...   \n",
       "17261433    200000   329705 2024-01-27 08:24:08-08:00   \n",
       "17261434    200000   408965 2024-01-27 09:38:34-08:00   \n",
       "17261435    200000   402449 2024-01-27 10:30:15-08:00   \n",
       "17261436    200000   337944 2024-01-27 13:21:36-08:00   \n",
       "17261437    200000  2885338 2024-01-27 14:26:30-08:00   \n",
       "\n",
       "                      end_datetime  \n",
       "0        2024-01-01 11:36:59-08:00  \n",
       "1        2024-01-01 12:33:47-08:00  \n",
       "2        2024-01-01 13:06:35-08:00  \n",
       "3        2024-01-01 14:05:50-08:00  \n",
       "4        2024-01-01 15:22:44-08:00  \n",
       "...                            ...  \n",
       "17261433 2024-01-27 09:24:42-08:00  \n",
       "17261434 2024-01-27 10:19:04-08:00  \n",
       "17261435 2024-01-27 13:07:01-08:00  \n",
       "17261436 2024-01-27 14:09:46-08:00  \n",
       "17261437 2024-01-29 07:34:54-08:00  \n",
       "\n",
       "[17261438 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poi_df.head()\n",
    "# get the number of unique values in each column\n",
    "cat_cols = ['latitude', 'longitude', 'poi_id', 'name']\n",
    "cat_dims = [train_poi_df[col].nunique() for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[686406, 686408, 800485, 237362]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a unique node index for each visit\n",
    "train_poi_df = train_poi_df.reset_index(drop=True)\n",
    "train_poi_df['node_idx'] = train_poi_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>act_types</th>\n",
       "      <th>node_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 00:00:00-08:00</td>\n",
       "      <td>2024-01-01 11:36:59-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>34.041928</td>\n",
       "      <td>-118.338327</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 12:48:23-08:00</td>\n",
       "      <td>2024-01-01 13:06:35-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>34.041928</td>\n",
       "      <td>-118.338327</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 14:21:28-08:00</td>\n",
       "      <td>2024-01-01 15:22:44-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>34.041928</td>\n",
       "      <td>-118.338327</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-01 17:32:54-08:00</td>\n",
       "      <td>2024-01-02 09:31:12-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>34.041928</td>\n",
       "      <td>-118.338327</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1518791</td>\n",
       "      <td>2024-01-02 13:51:05-08:00</td>\n",
       "      <td>2024-01-03 11:25:38-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>34.041928</td>\n",
       "      <td>-118.338327</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261433</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-24 16:22:26-08:00</td>\n",
       "      <td>2024-01-24 19:28:16-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>33.790039</td>\n",
       "      <td>-117.846742</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>17261433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261434</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-24 21:06:29-08:00</td>\n",
       "      <td>2024-01-25 06:27:41-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>33.790039</td>\n",
       "      <td>-117.846742</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>17261434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261435</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-25 15:18:47-08:00</td>\n",
       "      <td>2024-01-26 06:53:26-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>33.790039</td>\n",
       "      <td>-117.846742</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>17261435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261436</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-26 17:56:49-08:00</td>\n",
       "      <td>2024-01-27 08:08:48-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>33.790039</td>\n",
       "      <td>-117.846742</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>17261436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261437</th>\n",
       "      <td>200000</td>\n",
       "      <td>2885338</td>\n",
       "      <td>2024-01-27 14:26:30-08:00</td>\n",
       "      <td>2024-01-29 07:34:54-08:00</td>\n",
       "      <td>residence</td>\n",
       "      <td>33.790039</td>\n",
       "      <td>-117.846742</td>\n",
       "      <td>[1, 11, 15]</td>\n",
       "      <td>17261437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17261438 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agent_id   poi_id            start_datetime  \\\n",
       "0                1  1518791 2024-01-01 00:00:00-08:00   \n",
       "1                1  1518791 2024-01-01 12:48:23-08:00   \n",
       "2                1  1518791 2024-01-01 14:21:28-08:00   \n",
       "3                1  1518791 2024-01-01 17:32:54-08:00   \n",
       "4                1  1518791 2024-01-02 13:51:05-08:00   \n",
       "...            ...      ...                       ...   \n",
       "17261433    200000  2885338 2024-01-24 16:22:26-08:00   \n",
       "17261434    200000  2885338 2024-01-24 21:06:29-08:00   \n",
       "17261435    200000  2885338 2024-01-25 15:18:47-08:00   \n",
       "17261436    200000  2885338 2024-01-26 17:56:49-08:00   \n",
       "17261437    200000  2885338 2024-01-27 14:26:30-08:00   \n",
       "\n",
       "                      end_datetime       name   latitude   longitude  \\\n",
       "0        2024-01-01 11:36:59-08:00  residence  34.041928 -118.338327   \n",
       "1        2024-01-01 13:06:35-08:00  residence  34.041928 -118.338327   \n",
       "2        2024-01-01 15:22:44-08:00  residence  34.041928 -118.338327   \n",
       "3        2024-01-02 09:31:12-08:00  residence  34.041928 -118.338327   \n",
       "4        2024-01-03 11:25:38-08:00  residence  34.041928 -118.338327   \n",
       "...                            ...        ...        ...         ...   \n",
       "17261433 2024-01-24 19:28:16-08:00  residence  33.790039 -117.846742   \n",
       "17261434 2024-01-25 06:27:41-08:00  residence  33.790039 -117.846742   \n",
       "17261435 2024-01-26 06:53:26-08:00  residence  33.790039 -117.846742   \n",
       "17261436 2024-01-27 08:08:48-08:00  residence  33.790039 -117.846742   \n",
       "17261437 2024-01-29 07:34:54-08:00  residence  33.790039 -117.846742   \n",
       "\n",
       "            act_types  node_idx  \n",
       "0         [1, 11, 15]         0  \n",
       "1         [1, 11, 15]         1  \n",
       "2         [1, 11, 15]         2  \n",
       "3         [1, 11, 15]         3  \n",
       "4         [1, 11, 15]         4  \n",
       "...               ...       ...  \n",
       "17261433  [1, 11, 15]  17261433  \n",
       "17261434  [1, 11, 15]  17261434  \n",
       "17261435  [1, 11, 15]  17261435  \n",
       "17261436  [1, 11, 15]  17261436  \n",
       "17261437  [1, 11, 15]  17261437  \n",
       "\n",
       "[17261438 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1518791</td>\n",
       "      <td>residence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>1912553</td>\n",
       "      <td>residence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>103611</td>\n",
       "      <td>Chevron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>247124</td>\n",
       "      <td>Roving Karaoke Studio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>251512</td>\n",
       "      <td>Dixie Hollywood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261289</th>\n",
       "      <td>393516</td>\n",
       "      <td>Hastings Oaks Club House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261290</th>\n",
       "      <td>858075</td>\n",
       "      <td>residence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261355</th>\n",
       "      <td>156776</td>\n",
       "      <td>Petrozone gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261356</th>\n",
       "      <td>758742</td>\n",
       "      <td>residence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261405</th>\n",
       "      <td>2885338</td>\n",
       "      <td>residence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           poi_id                      name\n",
       "0         1518791                 residence\n",
       "1752      1912553                 residence\n",
       "2023       103611                   Chevron\n",
       "2324       247124     Roving Karaoke Studio\n",
       "2418       251512           Dixie Hollywood\n",
       "...           ...                       ...\n",
       "17261289   393516  Hastings Oaks Club House\n",
       "17261290   858075                 residence\n",
       "17261355   156776             Petrozone gas\n",
       "17261356   758742                 residence\n",
       "17261405  2885338                 residence\n",
       "\n",
       "[800485 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print corresponding names for each unique poi_id\n",
    "poi_names = train_poi_df[['poi_id', 'name']].drop_duplicates()\n",
    "poi_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "residence                                514177\n",
       "Starbucks                                   564\n",
       "7-Eleven                                    528\n",
       "Metabank                                    493\n",
       "ATM                                         461\n",
       "                                          ...  \n",
       "Garden Caf at Pilgrim Place                   1\n",
       "Garden Cafe at Norton Simon Museum            1\n",
       "Garden Center                                 1\n",
       "Garden Chapel (Faith Lutheran Church)         1\n",
       "                                           1\n",
       "Name: poi_id, Length: 237362, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of poi_ids for each unique 'name'\n",
    "poi_name_counts = train_poi_df.groupby('name')['poi_id'].nunique()\n",
    "\n",
    "# print the number of poi_ids for each unique 'name' descending\n",
    "poi_name_counts = poi_name_counts.sort_values(ascending=False)\n",
    "poi_name_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>act_types</th>\n",
       "      <th>node_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>11</td>\n",
       "      <td>324157</td>\n",
       "      <td>2024-01-14 20:06:10-08:00</td>\n",
       "      <td>2024-01-14 20:56:53-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.150546</td>\n",
       "      <td>-118.073148</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>10759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65179</th>\n",
       "      <td>110</td>\n",
       "      <td>325737</td>\n",
       "      <td>2024-01-02 09:10:46-08:00</td>\n",
       "      <td>2024-01-02 10:24:06-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.125039</td>\n",
       "      <td>-118.057616</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>65179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93927</th>\n",
       "      <td>163</td>\n",
       "      <td>149511</td>\n",
       "      <td>2024-01-23 11:01:51-08:00</td>\n",
       "      <td>2024-01-23 13:23:03-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>33.987014</td>\n",
       "      <td>-118.225554</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>93927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109948</th>\n",
       "      <td>193</td>\n",
       "      <td>209026</td>\n",
       "      <td>2024-01-07 18:18:11-08:00</td>\n",
       "      <td>2024-01-07 20:18:19-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.308749</td>\n",
       "      <td>-118.431488</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>109948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214880</th>\n",
       "      <td>386</td>\n",
       "      <td>356742</td>\n",
       "      <td>2024-01-05 18:16:52-08:00</td>\n",
       "      <td>2024-01-05 19:35:53-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>33.902705</td>\n",
       "      <td>-118.055044</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>214880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022681</th>\n",
       "      <td>193678</td>\n",
       "      <td>22641</td>\n",
       "      <td>2024-01-05 11:57:35-08:00</td>\n",
       "      <td>2024-01-05 13:24:42-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>33.759034</td>\n",
       "      <td>-117.989295</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>17022681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17034805</th>\n",
       "      <td>193996</td>\n",
       "      <td>276994</td>\n",
       "      <td>2024-01-03 13:06:37-08:00</td>\n",
       "      <td>2024-01-03 13:11:37-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.234422</td>\n",
       "      <td>-118.255933</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>17034805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116313</th>\n",
       "      <td>196159</td>\n",
       "      <td>395865</td>\n",
       "      <td>2024-01-03 18:33:24-08:00</td>\n",
       "      <td>2024-01-03 18:38:24-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.170039</td>\n",
       "      <td>-118.111773</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>17116313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17126662</th>\n",
       "      <td>196434</td>\n",
       "      <td>418176</td>\n",
       "      <td>2024-01-01 16:29:41-08:00</td>\n",
       "      <td>2024-01-01 17:32:12-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.581061</td>\n",
       "      <td>-118.141162</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>17126662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17228298</th>\n",
       "      <td>199107</td>\n",
       "      <td>266016</td>\n",
       "      <td>2024-01-07 17:12:20-08:00</td>\n",
       "      <td>2024-01-07 19:06:47-08:00</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>34.104123</td>\n",
       "      <td>-118.258925</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>17228298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agent_id  poi_id            start_datetime  \\\n",
       "10759           11  324157 2024-01-14 20:06:10-08:00   \n",
       "65179          110  325737 2024-01-02 09:10:46-08:00   \n",
       "93927          163  149511 2024-01-23 11:01:51-08:00   \n",
       "109948         193  209026 2024-01-07 18:18:11-08:00   \n",
       "214880         386  356742 2024-01-05 18:16:52-08:00   \n",
       "...            ...     ...                       ...   \n",
       "17022681    193678   22641 2024-01-05 11:57:35-08:00   \n",
       "17034805    193996  276994 2024-01-03 13:06:37-08:00   \n",
       "17116313    196159  395865 2024-01-03 18:33:24-08:00   \n",
       "17126662    196434  418176 2024-01-01 16:29:41-08:00   \n",
       "17228298    199107  266016 2024-01-07 17:12:20-08:00   \n",
       "\n",
       "                      end_datetime       name   latitude   longitude  \\\n",
       "10759    2024-01-14 20:56:53-08:00  Starbucks  34.150546 -118.073148   \n",
       "65179    2024-01-02 10:24:06-08:00  Starbucks  34.125039 -118.057616   \n",
       "93927    2024-01-23 13:23:03-08:00  Starbucks  33.987014 -118.225554   \n",
       "109948   2024-01-07 20:18:19-08:00  Starbucks  34.308749 -118.431488   \n",
       "214880   2024-01-05 19:35:53-08:00  Starbucks  33.902705 -118.055044   \n",
       "...                            ...        ...        ...         ...   \n",
       "17022681 2024-01-05 13:24:42-08:00  Starbucks  33.759034 -117.989295   \n",
       "17034805 2024-01-03 13:11:37-08:00  Starbucks  34.234422 -118.255933   \n",
       "17116313 2024-01-03 18:38:24-08:00  Starbucks  34.170039 -118.111773   \n",
       "17126662 2024-01-01 17:32:12-08:00  Starbucks  34.581061 -118.141162   \n",
       "17228298 2024-01-07 19:06:47-08:00  Starbucks  34.104123 -118.258925   \n",
       "\n",
       "         act_types  node_idx  \n",
       "10759       [2, 7]     10759  \n",
       "65179       [2, 7]     65179  \n",
       "93927       [2, 7]     93927  \n",
       "109948      [2, 7]    109948  \n",
       "214880      [2, 7]    214880  \n",
       "...            ...       ...  \n",
       "17022681    [2, 7]  17022681  \n",
       "17034805    [2, 7]  17034805  \n",
       "17116313    [2, 7]  17116313  \n",
       "17126662    [2, 7]  17126662  \n",
       "17228298    [2, 7]  17228298  \n",
       "\n",
       "[564 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take differnt entries with name \"Starbucks\" which have different poi_ids\n",
    "x= train_poi_df.groupby('name').get_group('Starbucks')\n",
    "\n",
    "# get unique poi_ids for x and print corresponding entries\n",
    "x.groupby('poi_id').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 2, 9, 15),\n",
       " (0, 2, 15),\n",
       " (1, 11, 15),\n",
       " (2,),\n",
       " (2, 3, 15),\n",
       " (2, 4, 15),\n",
       " (2, 5),\n",
       " (2, 5, 7),\n",
       " (2, 5, 7, 15),\n",
       " (2, 6, 14, 15),\n",
       " (2, 6, 15),\n",
       " (2, 7),\n",
       " (2, 7, 10, 15),\n",
       " (2, 8, 15),\n",
       " (2, 9),\n",
       " (2, 9, 10),\n",
       " (2, 9, 15),\n",
       " (2, 10, 15),\n",
       " (2, 12, 15),\n",
       " (2, 14, 15),\n",
       " (2, 15),\n",
       " (7,),\n",
       " (9, 10, 15),\n",
       " (9, 15),\n",
       " (10,),\n",
       " (13,),\n",
       " (14,),\n",
       " (14, 15)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all unique act_Types for train_poi_df\n",
    "act_types = train_poi_df['act_types']\n",
    "# Flatten and get unique values\n",
    "unique_act_types = set(tuple(sorted(sublist)) for sublist in act_types)\n",
    "unique_act_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m act_type_name \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act_type \u001b[38;5;129;01min\u001b[39;00m unique_act_types:\n\u001b[0;32m----> 4\u001b[0m     act_type_name[act_type] \u001b[38;5;241m=\u001b[39m train_poi_df[train_poi_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mact_types\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(act_type))][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      5\u001b[0m act_type_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m act_type_name \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act_type \u001b[38;5;129;01min\u001b[39;00m unique_act_types:\n\u001b[0;32m----> 4\u001b[0m     act_type_name[act_type] \u001b[38;5;241m=\u001b[39m train_poi_df[train_poi_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mact_types\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(act_type))][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      5\u001b[0m act_type_name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For each unique_act_types, get possible unique name \n",
    "act_type_name = {}\n",
    "for act_type in unique_act_types:\n",
    "    act_type_name[act_type] = train_poi_df[train_poi_df['act_types'].apply(lambda x: set(x) == set(act_type))]['name'].unique()\n",
    "act_type_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 4, 15): ['Work', 'ChildCare', 'DropOff'],\n",
       " (0, 2, 9, 15): ['Transportation', 'Work', 'Recreation', 'DropOff'],\n",
       " (2,): ['Work'],\n",
       " (2, 7, 10, 15): ['Work', 'EatOut', 'Exercise', 'DropOff'],\n",
       " (2, 8, 15): ['Work', 'Errands', 'DropOff'],\n",
       " (14,): ['SomethingElse'],\n",
       " (2, 10, 15): ['Work', 'Exercise', 'DropOff'],\n",
       " (2, 5, 7): ['Work', 'BuyGoods', 'EatOut'],\n",
       " (2, 5): ['Work', 'BuyGoods'],\n",
       " (2, 12, 15): ['Work', 'HealthCare', 'DropOff'],\n",
       " (2, 3, 15): ['Work', 'School', 'DropOff'],\n",
       " (2, 14, 15): ['Work', 'SomethingElse', 'DropOff'],\n",
       " (9, 10, 15): ['Recreation', 'Exercise', 'DropOff'],\n",
       " (2, 9, 10): ['Work', 'Recreation', 'Exercise'],\n",
       " (2, 6, 14, 15): ['Work', 'Services', 'SomethingElse', 'DropOff'],\n",
       " (7,): ['EatOut'],\n",
       " (2, 5, 7, 15): ['Work', 'BuyGoods', 'EatOut', 'DropOff'],\n",
       " (10,): ['Exercise'],\n",
       " (13,): ['Religious'],\n",
       " (14, 15): ['SomethingElse', 'DropOff'],\n",
       " (2, 7): ['Work', 'EatOut'],\n",
       " (0, 2, 15): ['Transportation', 'Work', 'DropOff'],\n",
       " (1, 11, 15): ['Home', 'Visit', 'DropOff'],\n",
       " (2, 9, 15): ['Work', 'Recreation', 'DropOff'],\n",
       " (9, 15): ['Recreation', 'DropOff'],\n",
       " (2, 9): ['Work', 'Recreation'],\n",
       " (2, 6, 15): ['Work', 'Services', 'DropOff'],\n",
       " (2, 15): ['Work', 'DropOff']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #- Transportation = 0\n",
    "# - 1 = Home\n",
    "# - 2 = Work\n",
    "# - 3 = School\n",
    "# - 4 = ChildCare\n",
    "# - 5 = BuyGoods\n",
    "# - 6 = Services\n",
    "# - 7 = EatOut\n",
    "# - 8 = Errands\n",
    "# - 9 = Recreation\n",
    "# - 10 = Exercise\n",
    "# - 11 = Visit\n",
    "# - 12 = HealthCare\n",
    "# - 13 = Religious\n",
    "# - 14 = SomethingElse\n",
    "# - 15 = DropOff\n",
    "\n",
    "# Each of the element in 1 act type is one of these numbers. Each number has a corresponding category name.\n",
    "# For example, 1 corresponds to 'Home', 2 corresponds to 'Work' and so on.\n",
    "\n",
    "# Map unique act types to corresponding category names\n",
    "list_of_categories = ['Transportation', 'Home', 'Work', 'School', 'ChildCare', 'BuyGoods', 'Services', 'EatOut', 'Errands', 'Recreation', 'Exercise', 'Visit', 'HealthCare', 'Religious', 'SomethingElse', 'DropOff']\n",
    "# Instead of numbers replace it with cateogry names for unique act types\n",
    "act_type_category = {}\n",
    "\n",
    "for act_type in unique_act_types:\n",
    "    act_type_category[act_type] = [list_of_categories[i] for i in act_type]\n",
    "\n",
    "# \n",
    "act_type_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act type category:  ['Work', 'ChildCare', 'DropOff']\n",
      "number of associated poi names:  13977\n",
      "act type category:  ['Transportation', 'Work', 'Recreation', 'DropOff']\n",
      "number of associated poi names:  272\n",
      "act type category:  ['Work']\n",
      "number of associated poi names:  7504\n",
      "act type category:  ['Work', 'EatOut', 'Exercise', 'DropOff']\n",
      "number of associated poi names:  15606\n",
      "act type category:  ['Work', 'Errands', 'DropOff']\n",
      "number of associated poi names:  4171\n",
      "act type category:  ['SomethingElse']\n",
      "number of associated poi names:  170\n",
      "act type category:  ['Work', 'Exercise', 'DropOff']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(2, 10, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act_type \u001b[38;5;129;01min\u001b[39;00m unique_act_types:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact type category: \u001b[39m\u001b[38;5;124m\"\u001b[39m, act_type_category[act_type])\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of associated poi names: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(act_type_name[act_type]))\n",
      "\u001b[0;31mKeyError\u001b[0m: (2, 10, 15)"
     ]
    }
   ],
   "source": [
    "# For the corresponding act type name, fetch the corresponding category name from act_type_category\n",
    "# display the number of associated names with each category\n",
    "\n",
    "for act_type in unique_act_types:\n",
    "    print(\"act type category: \", act_type_category[act_type])\n",
    "    print(\"number of associated poi names: \", len(act_type_name[act_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoding for act types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514177"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of unique poi_id for name 'residence'\n",
    "poi_names[poi_names['name'] == 'residence']\n",
    "# PRINT THE NUMBER OF UNIQUE POI_ID FOR NAME 'RESIDENCE'\n",
    "poi_names[poi_names['name'] == 'residence'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## whole dataset level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "# Download and load the model\n",
    "# load a 100 dimension word2vec model \n",
    "#word2vec_model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the poi names\n",
    "tokenized_names = [name.lower().split() for name in train_poi_df['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 32 dimension word2vec model on train_poi_df['name']\n",
    "# train the model on the tokenized names\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# if name_word2vec.model exists load it\n",
    "try:\n",
    "    model = Word2Vec.load(\"name_word2vec.model\")\n",
    "    print(\"Loaded model\")\n",
    "except:      \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        tokenized_names,\n",
    "        vector_size=32,  # Dimensionality of word vectors\n",
    "        window=3,        # ontext window size\n",
    "        min_count=1,     # Minimum count of occurrences for words\n",
    "        workers=4,       # Number of threads for training\n",
    "        sg=1             # Skip-gram model (set sg=0 for CBOW)\n",
    "    )\n",
    "\n",
    "\n",
    "    model.save(\"name_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimension of word2vec from 300 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "\n",
    "# # Extract word vectors from the Word2Vec model\n",
    "# word_vectors = word2vec_model.vectors\n",
    "\n",
    "# # Fit PCA to reduce dimensions to 32\n",
    "# pca = PCA(n_components=32)\n",
    "# reduced_word_vectors = pca.fit_transform(word_vectors)\n",
    "\n",
    "# # Create a dictionary to map words to their reduced embeddings\n",
    "# reduced_embeddings = {word: reduced_word_vectors[idx] for word, idx in word2vec_model.key_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# save_dir='./saved_embeddings'\n",
    "# # Create the directory if it does not exist\n",
    "# if not os.path.exists(save_dir):\n",
    "#    os.makedirs(save_dir)\n",
    "\n",
    "# # save the reduced dimension dictionary\n",
    "# import pickle\n",
    "# with open(f'{save_dir}/reduced_embeddings.pkl', 'wb') as f:\n",
    "#    pickle.dump(reduced_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the reduced embeddings pkl file\n",
    "# import pickle\n",
    "# save_dir='./saved_embeddings'\n",
    "# with open(f'{save_dir}/reduced_embeddings.pkl', 'rb') as f:\n",
    "#     reduced_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reduced embeddings to get the embedding for each name\n",
    "def get_embedding_reduced(name):\n",
    "    tokens = name.lower().split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in reduced_embeddings:\n",
    "            embeddings.append(reduced_embeddings[token])\n",
    "        else:\n",
    "            # Handle unknown tokens (e.g., use a zero vector or random vector)\n",
    "            embeddings.append(np.zeros(32))\n",
    "    # Average embeddings if multiple tokens\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"name_word2vec.model\")\n",
    "\n",
    "# Create a dictionary to map words to their embeddings\n",
    "reduced_embeddings = {word: word2vec_model.wv[word] for word in word2vec_model.wv.index_to_key}\n",
    "\n",
    "def get_embedding(name):\n",
    "    tokens = name.lower().split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[token])\n",
    "        else:\n",
    "            # Handle unknown tokens (e.g., use a zero vector or random vector)\n",
    "            embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "    # Average embeddings if multiple tokens\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding\n",
    "\n",
    "# Example usage\n",
    "name = \"residence\"\n",
    "embedding = get_embedding(name)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_encoder = LabelEncoder()\n",
    "train_poi_df['name_encoded'] = name_encoder.fit_transform(train_poi_df['name'])\n",
    "\n",
    "agent_encoder = LabelEncoder()\n",
    "train_poi_df['agent_id_encoded'] = agent_encoder.fit_transform(train_poi_df['agent_id'])\n",
    "\n",
    "poi_encoder = LabelEncoder()\n",
    "train_poi_df['poi_id_encoded'] = poi_encoder.fit_transform(train_poi_df['poi_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poi_df['duration'] = (train_poi_df['end_datetime'] - train_poi_df['start_datetime']).dt.total_seconds()\n",
    "# Convert duration from seconds to hours\n",
    "train_poi_df['duration_hour'] = (train_poi_df['duration'] / 3600).astype(int)\n",
    "lat_mean = train_poi_df['latitude'].mean()\n",
    "lat_std = train_poi_df['latitude'].std()\n",
    "lon_mean = train_poi_df['longitude'].mean()\n",
    "lon_std = train_poi_df['longitude'].std()\n",
    "duration_mean = train_poi_df['duration_hour'].mean()\n",
    "duration_std = train_poi_df['duration_hour'].std()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = train_poi_df[:1000].copy()\n",
    "data=train_poi_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# apply get embedding function to the name column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_embedding(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# apply get embedding function to the name column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_embedding(x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# apply get embedding function to the name column\n",
    "data['name_embedding'] = data['name'].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Encoding 'act_types' using multi-hot encoding\n",
    "# unique_act_types = set()\n",
    "# for acts in data['act_types']:\n",
    "#     unique_act_types.update(acts)\n",
    "# unique_act_types = list(unique_act_types)\n",
    "# act_type_to_idx = {act_type: idx for idx, act_type in enumerate(unique_act_types)}\n",
    "# act_type_features = torch.zeros((len(data), len(unique_act_types)))\n",
    "# for idx, acts in enumerate(data['act_types']):\n",
    "#     for act in acts:\n",
    "#         act_idx = act_type_to_idx[act]\n",
    "#         act_type_features[idx, act_idx] = 1  # Multi-hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save agent level graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll extract hour of day and day of week, and use cyclical encoding for hours\n",
    "def encode_time(dt_series):\n",
    "    # hours = dt_series.dt.hour + dt_series.dt.minute / 60.0\n",
    "    hours = dt_series.dt.hour\n",
    "    hours_norm = hours / 24.0 * 2 * math.pi  # Normalize to [0, 2pi]\n",
    "    hours_sin = np.sin(hours_norm)\n",
    "    hours_cos = np.cos(hours_norm)\n",
    "    \n",
    "    day_of_week = dt_series.dt.dayofweek  # Monday=0, Sunday=6\n",
    "    day_of_week_onehot = np.eye(7)[day_of_week]\n",
    "    day_norm = 2 * math.pi * day_of_week / 7.0\n",
    "    day_sin = np.sin(day_norm)\n",
    "    day_cos = np.cos(day_norm)\n",
    "    return hours_sin, hours_cos, day_sin, day_cos\n",
    "\n",
    "def get_features_per_agent(agent_df, lat_mean, lat_std, lon_mean, lon_std, duration_mean, duration_std):\n",
    "    agent_df['latitude_norm'] = (agent_df['latitude'] - lat_mean) / lat_std\n",
    "    agent_df['longitude_norm'] = (agent_df['longitude'] - lon_mean) / lon_std\n",
    "    # agent_df['duration_norm'] = (agent_df['duration_hour'] - duration_mean) / duration_std\n",
    "\n",
    "    # Encode start times\n",
    "    start_hours_sin, start_hours_cos, day_sin, day_cos = encode_time(agent_df['start_datetime'])\n",
    "    # Encode end times\n",
    "    end_hours_sin, end_hours_cos, ay_sin, day_cos = encode_time(agent_df['end_datetime'])\n",
    "    start_time_features = torch.tensor(\n",
    "        np.column_stack([start_hours_sin, start_hours_cos]),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    end_time_features = torch.tensor(\n",
    "        np.column_stack([end_hours_sin, end_hours_cos]),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # Prepare node features\n",
    "    # Numerical features\n",
    "    latitude = torch.tensor(agent_df['latitude_norm'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    longitude = torch.tensor(agent_df['longitude_norm'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    duration = torch.tensor(agent_df['duration_hour'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Categorical features as embeddings (here we'll just use the encoded values directly)\n",
    "    name_encoded = torch.tensor(agent_df['name_encoded'].values, dtype=torch.float32)\n",
    "    # name_embedding = torch.tensor(agent_df['name_embedding'], dtype=torch.float32)\n",
    "    name_embedding_values = agent_df['name_embedding'].apply(lambda x: np.array(x, dtype=np.float32) if isinstance(x, list) else x).values\n",
    "    name_embedding = torch.tensor(np.stack(name_embedding_values), dtype=torch.float32)\n",
    "\n",
    "    agent_id_encoded = torch.tensor(agent_df['agent_id_encoded'].values, dtype=torch.float32)\n",
    "    poi_id_encoded = torch.tensor(agent_df['poi_id_encoded'].values, dtype=torch.float32)\n",
    "    # Combine all node features\n",
    "    agent_node_features = torch.cat([\n",
    "        latitude,\n",
    "        longitude,\n",
    "        name_embedding,\n",
    "        poi_id_encoded.unsqueeze(1).float(),\n",
    "        start_time_features,\n",
    "        duration,\n",
    "    ], dim=1)\n",
    "\n",
    "    return agent_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_folder_name = 'mtm_all_features_stop_point_graphs_test'\n",
    "save_dir = f'/data/home/umang/Trajectory_project/anomaly_traj_data/haystac_anomaly_data1/saved_graphs/{save_folder_name}'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# save graph agent by agent\n",
    "agents = data['agent_id'].unique()\n",
    "\n",
    "# for agent_id in [1]:\n",
    "for agent_id in tqdm(agents):\n",
    "    edge_src = []\n",
    "    edge_dst = []\n",
    "    edge_time_diff = []\n",
    "    agent_data = data[data['agent_id'] == agent_id].sort_values('start_datetime')\n",
    "    \n",
    "    # Get all features first\n",
    "    agent_node_features = get_features_per_agent(agent_data, lat_mean, lat_std, lon_mean, lon_std, duration_mean, duration_std)\n",
    "    \n",
    "    # Create a mapping of feature tuple to node index\n",
    "    feature_to_node_idx = {}\n",
    "    node_indices = []\n",
    "    current_node_idx = 0\n",
    "    \n",
    "    # First pass: assign node indices based on unique features\n",
    "    for idx, features in enumerate(agent_node_features):\n",
    "        # Convert features to tuple for hashability\n",
    "        feature_tuple = tuple(features.tolist())\n",
    "        if feature_tuple not in feature_to_node_idx:\n",
    "            feature_to_node_idx[feature_tuple] = current_node_idx\n",
    "            current_node_idx += 1\n",
    "        node_indices.append(feature_to_node_idx[feature_tuple])\n",
    "    \n",
    "    # Create edges between consecutive visits\n",
    "    start_times = agent_data['start_datetime'].tolist()\n",
    "    for i in range(len(node_indices) - 1):\n",
    "        src_idx = node_indices[i]\n",
    "        dst_idx = node_indices[i + 1]\n",
    "        time_diff = (start_times[i + 1] - start_times[i]).total_seconds()\n",
    "        edge_src.append(src_idx)\n",
    "        edge_dst.append(dst_idx)\n",
    "        edge_time_diff.append(time_diff)\n",
    "\n",
    "    # Get unique features for the graph\n",
    "    unique_features = []\n",
    "    seen_features = set()\n",
    "    for features in agent_node_features:\n",
    "        feature_tuple = tuple(features.tolist())\n",
    "        if feature_tuple not in seen_features:\n",
    "            seen_features.add(feature_tuple)\n",
    "            unique_features.append(features)\n",
    "    unique_features = torch.stack(unique_features)\n",
    "    \n",
    "    G = dgl.graph((edge_src, edge_dst), num_nodes=len(feature_to_node_idx))\n",
    "    edge_time_diff_tensor = torch.tensor(edge_time_diff, dtype=torch.float32).unsqueeze(1)\n",
    "    G.edata['time_diff'] = edge_time_diff_tensor\n",
    "    G.ndata['attr'] = unique_features\n",
    "\n",
    "    # save graph as dgl graph\n",
    "    dgl.save_graphs(f'{save_dir}/agent_{agent_id}.dgl', G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one graph\n",
    "data_path = '/data/home/umang/Trajectory_project/anomaly_traj_data/haystac_anomaly_data1/saved_graphs'\n",
    "data_name = 'mtm_all_features_stop_point_graphs_test'\n",
    "agent_id = 1\n",
    "data_folder = f'{data_path}/{data_name}/agent_{agent_id}.dgl'\n",
    "G, _ = dgl.load_graphs(f'{data_folder}')\n",
    "G[0].ndata['attr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Steps TODO:\n",
    "\n",
    "# 1. Load the graph.\n",
    "# 2. Treat the graph as a trajectory of stop points.\n",
    "# 3. Distinguish features as part of \"observations\" and \"actions\".\n",
    "# 4. Store the agent movement as observations and actions in the buffer npz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all column names in train_poi_df\n",
    "train_poi_df.columns\n",
    "print(train_poi_df['agent_id'].nunique())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING MTM DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put agent id, poi id, latitude, longitude in observations\n",
    "\n",
    "# get all the unique agent ids\n",
    "unique_agent_ids = train_poi_df['agent_id'].unique()\n",
    "# for each unique agent id, get all corresponding poi ids, names, latitudes, longitudes and store it in the format of episode[observations]=shape (1001, 4), dtype float32\n",
    "\n",
    "# Initialize the observation dictionary\n",
    "observations = {}\n",
    "\n",
    "# Get unique agents\n",
    "agents = data['agent_id'].unique()\n",
    "\n",
    "# Iterate over each agent\n",
    "for agent_id in agents:\n",
    "    agent_data = data[data['agent_id'] == agent_id]\n",
    "    agent_id_append = np.full((len(agent_data), 1), agent_id)\n",
    "    # Extract POI IDs, names, latitudes, and longitudes\n",
    "    #poi_ids = agent_data['poi_id'].values\n",
    "    latitudes = agent_data['latitude'].values\n",
    "    longitudes = agent_data['longitude'].values\n",
    "    \n",
    "    # get start and end datetimes\n",
    "    start_datetimes = agent_data['start_datetime'].values\n",
    "    end_datetimes = agent_data['end_datetime'].values\n",
    "\n",
    "    # get if weekday or weekend\n",
    "    is_weekday = agent_data['start_datetime'].dt.dayofweek < 5\n",
    "    is_weekday = is_weekday.astype(int).values.reshape(-1, 1)\n",
    "\n",
    "    # get the day of the week, time of day, minute of the hour for start and end datetimes\n",
    "    start_hours = agent_data['start_datetime'].dt.hour.values.reshape(-1, 1)\n",
    "    start_minutes = agent_data['start_datetime'].dt.minute.values.reshape(-1, 1)\n",
    "    end_hours = agent_data['end_datetime'].dt.hour.values.reshape(-1, 1)\n",
    "    end_minutes = agent_data['end_datetime'].dt.minute.values.reshape(-1, 1)\n",
    "    start_days = agent_data['start_datetime'].dt.dayofweek.values.reshape(-1, 1)\n",
    "    end_days = agent_data['end_datetime'].dt.dayofweek.values.reshape(-1, 1)\n",
    "\n",
    "    # Combine the extracted data into a single array\n",
    "    combined_data = np.column_stack((agent_id_append, is_weekday, start_days, end_days,\n",
    "                                      start_hours, start_minutes, end_hours, end_minutes))\n",
    "        \n",
    "    # Store the combined data in the observations dictionary\n",
    "    observations[agent_id] = combined_data\n",
    "\n",
    "# Print the observations\n",
    "# for agent_id, obs in observations.items():\n",
    "#     print(f\"Agent ID: {agent_id}\")\n",
    "#     print(f\"Observations: shape {obs.shape}, dtype {obs.dtype}\")\n",
    "#     print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the actions dictionary\n",
    "actions = {}\n",
    "# for each unique agent id, get all corresponding poi ids, names, latitudes, longitudes and store it in the format of episode[observations]=shape (1001, 4), dtype float32\n",
    "\n",
    "# Get unique agents\n",
    "agents = data['agent_id'].unique()\n",
    "\n",
    "# Iterate over each agent\n",
    "for agent_id in agents:\n",
    "    agent_data = data[data['agent_id'] == agent_id]\n",
    "    \n",
    "    # get activity types for actions\n",
    "    actions[agent_id] = agent_data['act_types'].apply(lambda x: np.array(x, dtype=np.float32)).values\n",
    "\n",
    "# Print the actions\n",
    "for agent_id, act in actions.items():\n",
    "    print(f\"Agent ID: {agent_id}\")\n",
    "    print(f\"Actions: shape {act.shape}, dtype {act.dtype}\")\n",
    "    print(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the longest trajectory\n",
    "max_length = max(len(obs) for obs in observations.values())\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the observations and actions to the maximum length, for the padded regions, use zeros and create attention masks for padded regions\n",
    "# Initialize dictionaries to store padded observations and actions\n",
    "padded_observations = {}\n",
    "padded_actions = {}\n",
    "\n",
    "# Initialize dictionaries to store attention masks\n",
    "attention_masks = {}\n",
    "\n",
    "# Iterate over each agent\n",
    "for agent_id in agents:\n",
    "    # Get the observations and actions for the current agent\n",
    "    obs = observations[agent_id]\n",
    "    act = actions[agent_id]\n",
    "    \n",
    "    # Get the length of the current trajectory\n",
    "    length = len(obs)\n",
    "    \n",
    "    # Pad the observations and actions to the maximum length\n",
    "    padded_obs = np.zeros((max_length, obs.shape[1]), dtype=np.float32)\n",
    "    padded_act = np.zeros((max_length, act.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Create an attention mask for the padded regions\n",
    "    mask = np.zeros(max_length, dtype=np.float32)\n",
    "    \n",
    "    # Fill the padded observations and actions\n",
    "    padded_obs[:length] = obs\n",
    "    padded_act[:length] = act\n",
    "    mask[:length] = 1.0\n",
    "    \n",
    "    # Store the padded observations, actions, and attention masks\n",
    "    padded_observations[agent_id] = padded_obs\n",
    "    padded_actions[agent_id] = padded_act\n",
    "    attention_masks[agent_id] = mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK padded observations and actions and attention masks are all same shapes for all agents\n",
    "# for agent_id, obs in padded_observations.items():\n",
    "#     print(f\"Agent ID: {agent_id}\")\n",
    "#     print(f\"Padded Observations: shape {obs.shape}, dtype {obs.dtype}\")\n",
    "#     print(f\"Padded Actions: shape {act.shape}, dtype {act.dtype}\")\n",
    "#     print(f\"Attention Mask: shape {mask.shape}, dtype {mask.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the observations and actions in the buffer npz format\n",
    "# Save the observations and actions\n",
    "sub_dir='obs7_act1'\n",
    "save_dir = f'/data/home/umang/Trajectory_project/anomaly_traj_data/haystac_anomaly_data1/saved_agent_episodes_new/{sub_dir}'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinedly store the observations and actions as 1 agent episode in the buffer npz format\n",
    "for agent_id in agents:\n",
    "    obs = observations[agent_id]\n",
    "    act = actions[agent_id]\n",
    "    att_mask = attention_masks[agent_id]\n",
    "    np.savez(f'{save_dir}/agent_{agent_id}.npz', obs=obs, act=act, att_mask=att_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the npz file\n",
    "# Load the observations and actions\n",
    "agent_id = 1\n",
    "data_folder = f'{save_dir}/agent_{agent_id}.npz'\n",
    "loaded_data = np.load(data_folder)\n",
    "obs = loaded_data['obs']\n",
    "act = loaded_data['act']\n",
    "att_mask = loaded_data['att_mask']\n",
    "\n",
    "print(f\"Agent ID: {agent_id}\")\n",
    "print(f\"Observations: shape {obs.shape}, dtype {obs.dtype}\")\n",
    "print(obs)\n",
    "print(f\"Actions: shape {act.shape}, dtype {act.dtype}\")\n",
    "print(act)\n",
    "print(f\"Attention Masks: shape {att_mask.shape}, dtype {att_mask.dtype}\")\n",
    "print(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max and min for each dimension of the node features\n",
    "max_features = G[0].ndata['attr'].max(dim=0)[0]\n",
    "min_features = G[0].ndata['attr'].min(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 3 random words from word2vec and get their embeddings    \n",
    "import random\n",
    "random_words = random.sample(list(reduced_embeddings.keys()), 3)\n",
    "print(random_words)\n",
    "# get the embeddings\n",
    "random_embeddings = [reduced_embeddings[word] for word in random_words]\n",
    "\n",
    "# check the distance between all 3 embeddings using euclidean distance\n",
    "#from scipy.spatial.distance import cosine\n",
    "distances = []\n",
    "for i in range(3):\n",
    "    for j in range(i+1, 3):\n",
    "        distances.append(np.linalg.norm(random_embeddings[i] - random_embeddings[j]))\n",
    "        #distances.append(cosine(random_embeddings[i], random_embeddings[j]))\n",
    "print(distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which word in the dictionary is closest to the list of random embeddings\n",
    "min_dist = 100\n",
    "min_word = ''\n",
    "for rand_embedding in random_embeddings:\n",
    "    print(rand_embedding)\n",
    "for rand_embedding in random_embeddings:\n",
    "    rand_embedding_tensor = torch.tensor(rand_embedding)\n",
    "    for word in reduced_embeddings.keys():\n",
    "        embedding_tensor = torch.tensor(reduced_embeddings[word])\n",
    "\n",
    "        # use cosine similarity to get the distance between the embeddings\n",
    "        dist = torch.nn.functional.cosine_similarity(embedding_tensor, rand_embedding_tensor, dim=0)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_word = word\n",
    "    print(min_dist, min_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Prepare data\n",
    "keys = list(embedding_tensor.keys())\n",
    "dim = len(embedding_tensor[keys[0]])\n",
    "index = AnnoyIndex(dim, 'euclidean')\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    index.add_item(i, embedding_tensor[key])\n",
    "\n",
    "# Build the index\n",
    "index.build(10)\n",
    "\n",
    "# Perform search\n",
    "nearest_idx = index.get_nns_by_vector(search_vector, 1)[0]\n",
    "closest_key = keys[nearest_idx]\n",
    "\n",
    "print(\"Closest Vector Key:\", closest_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of files in the saved directory\n",
    "import os\n",
    "sub_dir='obs4_act1'\n",
    "save_dir = f'/data/home/umang/Trajectory_project/anomaly_traj_data/haystac_anomaly_data1/saved_agent_episodes/{sub_dir}'\n",
    "len(os.listdir(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved npz files\n",
    "# get the file names with minimum and maximum rows (Stop points) in the observations\n",
    "import numpy as np\n",
    "min_rows = float('inf')\n",
    "max_rows = 0\n",
    "min_file = ''\n",
    "max_file = ''\n",
    "for file in os.listdir(save_dir):\n",
    "    data = np.load(f'{save_dir}/{file}')\n",
    "    obs = data['obs']\n",
    "    if obs.shape[0] < min_rows:\n",
    "        min_rows = obs.shape[0]\n",
    "        min_file = file\n",
    "    if obs.shape[0] > max_rows:\n",
    "        max_rows = obs.shape[0]\n",
    "        max_file = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the file names with minimum and maximum rows\n",
    "print(f\"File with minimum rows: {min_file}, rows: {min_rows}\")\n",
    "print(f\"File with maximum rows: {max_file}, rows: {max_rows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
