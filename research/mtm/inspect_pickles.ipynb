{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"/data/home/umang/Trajectory_project/GPS-MTM/outputs/test_haystac/2025-08-28_18-13-53/test_outputs/random_masking_0.5_testing/ID\"\n",
    "pred_pickle_path=\"{}/predictions_batch_5000.pkl\".format(folder)\n",
    "batch_pickle_path = \"{}/ground_truth_batch_5000.pkl\".format(folder)\n",
    "attention_pickle_path = \"{}/attention_masks_batch_5000.pkl\".format(folder)\n",
    "masks_pickle_path = \"{}/masks_batch_0.pkl\".format(folder)\n",
    "with open(pred_pickle_path, 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "with open(batch_pickle_path, 'rb') as f:\n",
    "    batch = pickle.load(f)\n",
    "\n",
    "with open(attention_pickle_path, 'rb') as f:\n",
    "    attention_masks = pickle.load(f)\n",
    "\n",
    "with open(masks_pickle_path, 'rb') as f:\n",
    "    masks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([221])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks['states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 221, 28])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 221, 11])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"actions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most frequently top-5 most repeated in ground_truth_list_States\n",
    "# #don't use torch top-k..\n",
    "ground_truth_list_states = []\n",
    "for i in range(len(attention_masks)):\n",
    "     # first zero mask is first time when attention_masks[i] becomes zero\n",
    "    zero_indices = (attention_masks[i].flatten() == 0).nonzero(as_tuple=True)[0]\n",
    "    first_zero_mask = zero_indices[0].item() if len(zero_indices) > 0 else attention_masks[i].numel()\n",
    "    \n",
    "    ground_truth_states= batch[\"states\"][i, :first_zero_mask, :]\n",
    "    ground_truth_states = torch.argmax(ground_truth_states, dim=-1)\n",
    "    ground_truth_list_states.append(ground_truth_states)\n",
    "\n",
    "k=4\n",
    "ground_truth_list_states = torch.cat(ground_truth_list_states).flatten()\n",
    "unique, counts = torch.unique(ground_truth_list_states, return_counts=True)\n",
    "top_k_ground_truth = unique[torch.topk(counts, k=k).indices]\n",
    "top_k_ground_truth\n",
    "\n",
    "top_1_ground_truth = unique[torch.topk(counts, k=1).indices]\n",
    "top_4_ground_truth = unique[torch.topk(counts, k=4).indices]    \n",
    "# top_5_ground_truth = unique[torch.topk(counts, k=5).indices]\n",
    "# top_10_ground_truth = unique[torch.topk(counts, k=10).indices]\n",
    "# top_20_ground_truth = unique[torch.topk(counts, k=20).indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.4002e+04,  1.0000e+00,  0.0000e+00,  ...,  9.7963e-01,\n",
       "         -1.8511e+00, -7.0780e-03],\n",
       "        [ 6.4002e+04,  1.0000e+00,  0.0000e+00,  ...,  1.2759e+00,\n",
       "         -1.2606e+00, -7.0780e-03],\n",
       "        [ 6.4002e+04,  1.0000e+00,  0.0000e+00,  ...,  9.7963e-01,\n",
       "         -1.8511e+00, -6.6465e-01],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['actions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'states': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.], dtype=torch.float64),\n",
       " 'actions': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "predictions_list_states=[]\n",
    "predictions_list_actions=[]\n",
    "ground_truth_list_states=[]\n",
    "ground_truth_list_actions=[]\n",
    "correct_list=[]\n",
    "\n",
    "\n",
    "# Get mask for this sequence\n",
    "mask_states = masks[\"states\"] == 1\n",
    "mask_actions = masks[\"actions\"] == 1\n",
    "\n",
    "net_masks_states= attention_masks* masks[\"states\"]\n",
    "net_masks_actions= attention_masks* masks[\"actions\"]\n",
    "\n",
    "# Apply masks to get only positions where mask is 1\n",
    "predictions_states = predictions[\"states\"][net_masks_states.bool(), :]\n",
    "predictions_actions = predictions[\"actions\"][net_masks_actions.bool(), :]\n",
    "ground_truth_states = batch[\"states\"][net_masks_states.bool(), :]\n",
    "ground_truth_actions = batch[\"actions\"][net_masks_actions.bool(), :]\n",
    "\n",
    "# take argmax of predictions states on last dimension\n",
    "\n",
    "predictions_list_states.append(torch.argmax(predictions_states, dim=-1))\n",
    "predictions_list_actions.append(torch.argmax(predictions_actions, dim=-1))\n",
    "ground_truth_list_states.append(torch.argmax(ground_truth_states, dim=-1))\n",
    "ground_truth_list_actions.append(torch.argmax(ground_truth_actions, dim=-1))\n",
    "\n",
    "print(ground_truth_states)\n",
    "correct = (predictions_states == ground_truth_states).float()\n",
    "correct_list.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_masks_states[..., 0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.9841,  -7.1141,  -3.8031,  -0.6775,  -3.4464,  -7.6579,  -2.4293,\n",
       "          -4.3889,  -7.2977,  -2.4098,  -2.2652,  -9.5652,  -3.5005,  -2.0540,\n",
       "          -4.8445,  -9.4350,   0.4818,  -9.0604,  -1.9972,  -9.4097,  -2.5872,\n",
       "         -13.3960,   3.5602,  -3.5836,  -3.4553,  -0.4803,  -2.5592,  -3.9427],\n",
       "        [ -2.5780,  -7.0448,  -2.7969,  -1.1122,  -2.6556,  -8.0755,  -1.9673,\n",
       "          -5.1917,  -6.9576,  -1.5807,  -1.5871,  -9.1667,  -4.3079,  -2.2315,\n",
       "          -4.2874, -10.3568,   0.2844,  -8.1788,  -1.1772,  -9.2855,  -2.4063,\n",
       "         -11.5044,   1.9791,  -2.1299,  -3.8009,  -0.2547,  -1.9919,  -2.8733],\n",
       "        [ -1.0039,  -6.1492,  -2.2548,  -0.0539,  -1.4674,  -7.2733,  -1.0231,\n",
       "          -3.2229,  -3.4613,  -1.0903,  -0.8755,  -6.5588,  -4.9496,  -2.0548,\n",
       "          -3.1250,  -8.6848,   1.1061,  -6.4396,  -2.0252,  -7.1727,   0.0882,\n",
       "         -10.2501,   3.3186,  -1.4977,  -3.2768,  -0.7507,  -1.7184,  -2.0916],\n",
       "        [ -2.3711,  -4.3417,  -1.0993,   0.8341,  -0.3443,  -4.4085,  -0.4077,\n",
       "          -3.1178,  -3.3300,  -0.3263,  -0.1118,  -5.9475,  -2.6702,  -1.1350,\n",
       "          -1.4807,  -6.0431,   1.8950,  -4.8311,   0.3232,  -4.8204,   0.5558,\n",
       "          -8.9283,   3.5443,  -0.4248,  -1.6353,   0.1934,  -0.9628,  -2.2978],\n",
       "        [ -1.2003,  -4.6426,  -0.8671,   0.4640,  -0.3289,  -3.7365,  -0.6075,\n",
       "          -3.0380,  -3.0184,   0.2079,   1.0909,  -5.0498,  -2.9038,  -1.6361,\n",
       "          -1.3363,  -6.5903,   1.5724,  -5.0876,  -0.8537,  -3.8296,   0.1682,\n",
       "          -7.8373,   3.4905,  -1.0568,  -1.7811,  -0.2871,  -0.7113,  -1.3348]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_states[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([44, 28]), torch.Size([44, 28]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_states.shape,ground_truth_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([44]), torch.Size([44]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list_states=torch.cat(predictions_list_states).flatten()\n",
    "ground_truth_list_states=torch.cat(ground_truth_list_states).flatten()\n",
    "predictions_list_states.shape, ground_truth_list_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 16,\n",
       "         16, 16, 22, 22, 22, 22, 22, 16]),\n",
       " tensor([22, 10, 22,  6, 22, 10, 22,  0, 22, 10, 22, 10,  9, 22, 18, 22, 22, 22,\n",
       "          4,  9, 16, 22, 22, 22, 10, 22, 10, 22, 22, 10,  9, 22,  0, 22, 10, 22,\n",
       "         22, 16, 22, 16, 22, 10, 22, 10]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list_states[0:200], ground_truth_list_states[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(net_masks_states[..., 0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 12,  0, 16,  0, 12,  0, 22,  0, 12,  0, 12, 13,  0,  4,  0,  0,  0,\n",
       "        18, 13,  6,  0,  0,  0, 12,  0, 12,  0,  0, 12, 13,  0, 22,  0, 12, -6,\n",
       "        -6,  0,  0,  6,  0, 12,  0,  6])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list_states[0:200]-ground_truth_list_states[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "## Calculate accuracy USING predicted states and ground truth list states.\n",
    "total_correct = 0\n",
    "total_elements = 0\n",
    "for pred_states, gt_states in zip(predictions_list_states, ground_truth_list_states):\n",
    "    correct = (pred_states == gt_states).float()\n",
    "    total_correct += correct.sum().item()\n",
    "    total_elements += correct.numel()\n",
    "accuracy = total_correct / total_elements if total_elements > 0 else 0\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Count  Accuracy Precision Recall F1    \n",
      "--------------------------------------------------\n",
      "22    23     0.9130   0.5250    0.9130 0.6667\n",
      "10    10     0.0000   0.0000    0.0000 0.0000\n",
      "9     3      0.0000   0.0000    0.0000 0.0000\n",
      "16    3      0.3333   0.2500    0.3333 0.2857\n",
      "0     2      0.0000   0.0000    0.0000 0.0000\n",
      "4     1      0.0000   0.0000    0.0000 0.0000\n",
      "6     1      0.0000   0.0000    0.0000 0.0000\n",
      "18    1      0.0000   0.0000    0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "# filter predictions and ground truth for each unique class\n",
    "unique_classes = torch.unique(ground_truth_list_states)\n",
    "filtered_predictions = {}\n",
    "filtered_ground_truth = {}\n",
    "for cls in unique_classes:\n",
    "    mask = (ground_truth_list_states == cls)\n",
    "    filtered_predictions[cls.item()] = predictions_list_states[mask]\n",
    "    filtered_ground_truth[cls.item()] = ground_truth_list_states[mask]\n",
    "\n",
    "# sort classes by frequency\n",
    "class_counts = []\n",
    "for cls in unique_classes:\n",
    "    count = (ground_truth_list_states == cls).sum().item()\n",
    "    class_counts.append((cls.item(), count))\n",
    "\n",
    "# sort by count in descending order\n",
    "class_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# compute and print accuracy, precision, recall, and f1 score for each class sorted by frequency\n",
    "print(f\"{'Class':<5} {'Count':<6} {'Accuracy':<8} {'Precision':<9} {'Recall':<6} {'F1':<6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for cls, count in class_counts:\n",
    "    # Get predictions and ground truth for this class\n",
    "    cls_predictions = filtered_predictions[cls]\n",
    "    cls_ground_truth = filtered_ground_truth[cls]\n",
    "    \n",
    "    # Accuracy for this class\n",
    "    accuracy = (cls_predictions == cls_ground_truth).float().mean().item()\n",
    "    \n",
    "    # For precision, recall, F1: need to consider this class vs all others\n",
    "    # True positives: predicted this class and actually this class\n",
    "    tp = ((predictions_list_states == cls) & (ground_truth_list_states == cls)).sum().item()\n",
    "    \n",
    "    # False positives: predicted this class but actually not this class\n",
    "    fp = ((predictions_list_states == cls) & (ground_truth_list_states != cls)).sum().item()\n",
    "    \n",
    "    # False negatives: didn't predict this class but actually this class\n",
    "    fn = ((predictions_list_states != cls) & (ground_truth_list_states == cls)).sum().item()\n",
    "    \n",
    "    # Calculate precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    print(f\"{cls:<5} {count:<6} {accuracy:<8.4f} {precision:<9.4f} {recall:<6.4f} {f1:<6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Metrics:\n",
      "Overall Accuracy: 0.5000\n",
      "Macro F1 Score: 0.1190\n",
      "Weighted F1 Score: 0.3680\n",
      "Macro Precision: 0.0969\n",
      "Weighted Precision: 0.2915\n",
      "Macro Recall: 0.1558\n",
      "Weighted Recall: 0.5000\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         3\n",
      "          10       0.00      0.00      0.00        10\n",
      "          16       0.25      0.33      0.29         3\n",
      "          18       0.00      0.00      0.00         1\n",
      "          22       0.53      0.91      0.67        23\n",
      "\n",
      "    accuracy                           0.50        44\n",
      "   macro avg       0.10      0.16      0.12        44\n",
      "weighted avg       0.29      0.50      0.37        44\n",
      "\n",
      "\n",
      "Class Count  Freq%  Precision Recall F1    \n",
      "--------------------------------------------------\n",
      "22    23     52.3   0.5250    0.9130 0.6667\n",
      "10    10     22.7   0.0000    0.0000 0.0000\n",
      "9     3      6.8    0.0000    0.0000 0.0000\n",
      "16    3      6.8    0.2500    0.3333 0.2857\n",
      "0     2      4.5    0.0000    0.0000 0.0000\n",
      "4     1      2.3    0.0000    0.0000 0.0000\n",
      "6     1      2.3    0.0000    0.0000 0.0000\n",
      "18    1      2.3    0.0000    0.0000 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/umang/miniconda3/envs/mtm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/data/home/umang/miniconda3/envs/mtm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/data/home/umang/miniconda3/envs/mtm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/data/home/umang/miniconda3/envs/mtm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/data/home/umang/miniconda3/envs/mtm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Convert to numpy for sklearn compatibility\n",
    "gt_numpy = ground_truth_list_states.cpu().numpy()\n",
    "pred_numpy = predictions_list_states.cpu().numpy()\n",
    "\n",
    "# Overall metrics\n",
    "overall_accuracy = accuracy_score(gt_numpy, pred_numpy)\n",
    "macro_f1 = f1_score(gt_numpy, pred_numpy, average='macro')\n",
    "weighted_f1 = f1_score(gt_numpy, pred_numpy, average='weighted')\n",
    "macro_precision = precision_score(gt_numpy, pred_numpy, average='macro')\n",
    "weighted_precision = precision_score(gt_numpy, pred_numpy, average='weighted')\n",
    "macro_recall = recall_score(gt_numpy, pred_numpy, average='macro')\n",
    "weighted_recall = recall_score(gt_numpy, pred_numpy, average='weighted')\n",
    "\n",
    "print(\"Overall Performance Metrics:\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"Weighted Recall: {weighted_recall:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(gt_numpy, pred_numpy))\n",
    "\n",
    "# Class-wise metrics accounting for imbalance\n",
    "unique_classes = torch.unique(ground_truth_list_states)\n",
    "class_counts = [(cls.item(), (ground_truth_list_states == cls).sum().item()) for cls in unique_classes]\n",
    "class_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Class':<5} {'Count':<6} {'Freq%':<6} {'Precision':<9} {'Recall':<6} {'F1':<6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for cls, count in class_counts:\n",
    "    freq_pct = (count / len(ground_truth_list_states)) * 100\n",
    "    \n",
    "    # Calculate metrics for this specific class\n",
    "    cls_mask_gt = (gt_numpy == cls)\n",
    "    cls_mask_pred = (pred_numpy == cls)\n",
    "    \n",
    "    tp = ((pred_numpy == cls) & (gt_numpy == cls)).sum()\n",
    "    fp = ((pred_numpy == cls) & (gt_numpy != cls)).sum()\n",
    "    fn = ((pred_numpy != cls) & (gt_numpy == cls)).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    print(f\"{cls:<5} {count:<6} {freq_pct:<6.1f} {precision:<9.4f} {recall:<6.4f} {f1:<6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct predictions in tensor([22]) ground truth: 21\n",
      "Accuracy of correct predictions in tensor([22]) ground truth: 0.9130434989929199\n",
      "Number of correct predictions in tensor([22, 10, 16,  9]) ground truth: 22\n",
      "Accuracy of correct predictions in tensor([22, 10, 16,  9]) ground truth: 0.5641025900840759\n"
     ]
    }
   ],
   "source": [
    "# get the number of correct predictions for classes in top_10_ground_truth\n",
    "#top_k=[top_1_ground_truth, top_5_ground_truth, top_10_ground_truth, top_20_ground_truth]\n",
    "top_k=[top_1_ground_truth, top_4_ground_truth]\n",
    "for i in range(len(top_k)):\n",
    "    filtered_gt = ground_truth_list_states[(ground_truth_list_states.unsqueeze(-1) == top_k[i]).any(dim=-1)]\n",
    "    filtered_pred = predictions_list_states[(ground_truth_list_states.unsqueeze(-1) == top_k[i]).any(dim=-1)]\n",
    "\n",
    "    # get the number of correct predictions and accuracy\n",
    "    correct_predictions = (filtered_pred == filtered_gt).sum()\n",
    "    accuracy = correct_predictions / filtered_gt.numel()\n",
    "    print(f\"Number of correct predictions in {top_k[i]} ground truth: {correct_predictions.item()}\")\n",
    "    print(f\"Accuracy of correct predictions in {top_k[i]} ground truth: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that model is working properly below.\n",
    "\n",
    "-0.0207 is the index for mask value to be filled in. Corresponding mask is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pdb) x_sample[0,:,0]\n",
    "# tensor([-0.9309, -2.4709, -0.9647, -2.5226, -0.9906, -2.6224, -1.0323, -2.6396,\n",
    "#         -1.0287, -2.5965, -0.0207, -0.0207, -1.0252, -2.4983, -1.0098, -0.0207,\n",
    "#         -1.0417, -2.3514, -0.0207, -0.0207, -0.0207, -2.3262, -0.9608, -2.4531,\n",
    "#         -0.9609, -2.5430, -0.0207, -2.4288, -0.0207, -1.5484, -0.9251, -2.4177,\n",
    "#         -0.0207, -0.0207, -0.8637, -2.3879, -0.8872, -0.0207, -0.9029, -2.3117,\n",
    "#         -0.9281, -2.3534, -1.0126, -2.4598, -0.9165, -0.0207, -0.8610, -2.4906,\n",
    "#         -0.0207, -2.5534, -1.0234, -0.8551, -2.4868, -0.8567, -2.4971, -0.9002,\n",
    "#         -2.4162, -0.0207, -0.0207, -0.9232, -0.0207, -0.9814, -0.0207, -0.0207,\n",
    "#         -2.3403, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207, -0.0207,\n",
    "#         -0.0207, -0.0207, -0.0207, -0.0207, -0.0207], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pdb) masks['states']\n",
    "# tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
    "#         1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
    "#         0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "#         1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "#         0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
    "#         1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 0, 1, 1], device='cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
